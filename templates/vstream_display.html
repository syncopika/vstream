<html>

	<head>
		<title>vstream display</title>
		<script src="//cdnjs.cloudflare.com/ajax/libs/socket.io/2.2.0/socket.io.js" integrity="sha256-yr4fRk/GU1ehYJPAs8P4JlTgu0Hdsp4ZKrx8bDEDC3I=" crossorigin="anonymous"></script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/three.js/108/three.min.js'></script>
		<script src='static/GLTFLoader.js'></script>
	</head>
	
	<style>
		#main{
			font-family: monospace;
			text-align: center;
			top: 5%;
			position: relative;
			display: block;
		}
		canvas{
			border: #000 solid 1px;
		}
		#container{
			width: 400px;
			height: 400px;
			margin: 0 auto;
			display: inline-block;
		}
	</style>
	
	<body>
		<div id='main'>
			<h3> vstream display </h3>
			
			<div id='show'>
				<canvas id='display' width='400px' height='400px'>
				</canvas>
			
				<div id='container'>
				</div>
			</div>
			
			<br />
			<br />
			
			<button id='calibrate'> calibrate </button>
			<button id='toggleStream'> pause/continue </button>
			<!--<button id='toggleWireframe'> toggle wireframe </button>-->
			<br />
			
			<h3 id='mouthCheck'> this is a test </h3>
			<h3 id='mouthCheck2'> this is a test </h3>
			<!--
			<h3 id='currAnchorPosition'></h3>
			<br />
			
			<h3 id='currentJawDist'></h3>
			<br />
			
			<h2> translation: </h2>
			<h3 id='currentAction'></h3>
			<br />
			
			<h2> z-axis rotation: </h2>
			<h3 id='zAxisRotation'></h3>
			<br />
			
			<h3 id='zAxisRotation2'></h3>
			<br />
			
			<h2> y-axis rotation: </h2>
			<h3 id='yAxisRotation'></h3>
			<br />
			-->
			
		</div>
	</body>


	<script>
	
		/* notes 
		
			oooh this looks good: https://github.com/auduno/clmtrackr
		
			potentially useful resources (mostly about morphing, but the warping step I think is really what we want):
			https://www.cs.toronto.edu/~mangas/teaching/320/slides/CSC320T12.pdf
			https://www.cs.toronto.edu/~mangas/teaching/320/slides/CSC320T12.pdf
			https://pdfs.semanticscholar.org/f528/d3e9ca5b8bf7e3b47aadce9f151a6467150a.pdf
			https://www.cs.uaf.edu/~olawlor/academic/thesis/ref/chen93interpolation.pdf -> 2.2 looks very relevant
			http://eeweb.poly.edu/~yao/EL5123/lecture12_ImageWarping.pdf
			
			after more reading, not sure the info on warping is super helpful... :/
			but, I think when I calibrate, I'll have to do some information gathering from the landmark coords. 
			I just don't know right now exactly what I need.
		*/
	
		const socket = io();
		let streamPaused = false;
		let calibrate = false;
		let showWireframe = true; //false;
		
		// maybe instead of anchor points, we just save all the points of the previous frame to compare with. :>
		// let's use landmark coordinate number 27 and 30 (that's part of the nose)
		let anchorPoint = null; // should be a map with just x and y as keys, i.e. {'x': 0, 'y': 0}
		let anchorPoint2 = null; // use this point to create a 'normal' vector with anchorPoint.
		let lastJawEndptDist = null;
		let rightJawEnd = null;
		let leftJawEnd = null;
		let updateAnchorPoint = false;
		let prevAnchor = null;
		let currAnchor = null;
		
		const baselineMouthCalibration = {
			"top2d": {}, // x,y coord
			"bottom2d": {},
			"left2d": {}, // left corner 
			"right2d": {},
			"dist2d": 0, // distance between the top and bottom pts for the 2d canvas (landmark pts)
			"left3d": {},
			"right3d": {}
			//"dist3d": 0 // distance between the top and bottom pts for the model
		};
		
		function toggleStream(){
			streamPaused = !streamPaused;
			socket.emit("toggleStream", streamPaused);
		}
		
		document.getElementById("toggleStream").addEventListener("click", (evt) => {
			console.log("toggling stream...");
			toggleStream();
		});
		

		function getDistance(point1, point2){
			return Math.sqrt(Math.pow((point2.x - point1.x),2) + Math.pow((point2.y - point1.y),2));
		}
		
		// pass in 2 coordinates
		// https://stackoverflow.com/questions/14066933/direct-way-of-computing-clockwise-angle-between-2-vectors
		function getAngle(vec1, vec2){
			let dot = (vec1.x * vec2.x) + (vec1.y * vec2.y);
			let det = (vec1.x * vec2.y) - (vec1.y * vec2.x);
			let angle = Math.atan2(det, dot) * (180 / Math.PI);
			return angle;
		}
		
		// set the initial nose/anchor point that we can use to figure out when a rotation is happening
		function calibrateAnchor(pt1, pt2){
			// using global vars here 
			anchorPoint = pt1;
			anchorPoint2 = pt2;
		}
		
		function calibrateMouthBaseline(top2d, bottom2d, left2d, right2d, top3d, bottom3d, left3d, right3d){
			// this is ref'ing a global var. amaybe pass as param instead?
			
			// these coords are kinda useless when a rotation occurs
			// but I think we can assume some things. when a rotation occurs,
			// we can always get the position of the top mouth coord for free 
			// and since distance is preserved as well, we can figure out where 
			// the bottom mouth coord should be given that info so we'll always have a baseline (i.e. position of mouth coords for a closed mouth position)
			baselineMouthCalibration.top2d = top2d;
			baselineMouthCalibration.bottom2d = bottom2d;
			baselineMouthCalibration.left2d = left2d;
			baselineMouthCalibration.right2d = right2d;
			baselineMouthCalibration.left3d = left3d;
			baselineMouthCalibration.right3d = right3d;
			//baselineMouthCalibration.top3d = top3d;
			//baselineMouthCalibration.bottom3d = bottom3d;
			
			baselineMouthCalibration.dist2d = getDistance(top2d, bottom2d);
			//baselineMouthCalibration.dist3d = getDistance(top3d, bottom3d);
			
			document.getElementById('mouthCheck').textContent = "left mouth corner x = baseline 3d: " + baselineMouthCalibration.left3d.x;  
			document.getElementById('mouthCheck2').textContent = "right mouth corner x = baseline 3d: " + baselineMouthCalibration.right3d.x;  
		}
		
		function inRange(x1, x2, rangeLimit){
			return (x1 <= x2 + rangeLimit) && (x1 >= x2 - rangeLimit);
		}
		
		document.getElementById('calibrate').addEventListener('click', (evt) => {
			calibrate = !calibrate;
		});
		
		/*
		document.getElementById('toggleWireframe').addEventListener('click', (evt) => {
			showWireframe = !showWireframe;
		});*/
		
		
		//////////////////////////////////////////// three js stuff
		
		function getModel(modelFilePath, name){
			return new Promise((resolve, reject) => {
				loader.load(
					modelFilePath,
					function(gltf){
						if(gltf.animations.length > 0){
							console.log(gltf.animations);
						}
						var count = 0;
						var faceParts = [];
						gltf.scene.traverse((child) => {

							if(child.type === "Mesh"){
								
								var name = "mesh" + count;
								
								let material = child.material;
								let geometry = child.geometry;
								//console.log(geometry);
								
								// https://stackoverflow.com/questions/52569738/how-to-access-single-vertices-of-mesh-loaded-with-gltfloader-in-three-js
								let obj = new THREE.Mesh(geometry, material);
								obj.scale.x = child.scale.x * 5;
								obj.scale.y = child.scale.y * 5;
								obj.scale.z = child.scale.z * 5;
								obj.name = name;
								count++;
								//resolve(obj);
								faceParts.push(obj);
								
							}else{
								//console.log(child.type);
							}
						});
						resolve(faceParts);
					},
					// called while loading is progressing
					function(xhr){
						console.log( (xhr.loaded / xhr.total * 100) + '% loaded' );
					},
					// called when loading has errors
					function(error){
						console.log('An error happened');
						console.log(error);
					}
				);
			});
		}
		
		const loader = new THREE.GLTFLoader();
		let loadedModels = [];
		
		const avatarParts = {
			"head": null,
			"leftEyebrow": null,
			"leftEye": null,
			"rightEyebrow": null,
			"rightEye": null,
			"mouth": null
		};
		
		const group = new THREE.Group();
		let meshLoaded = false;
		
		const el = document.getElementById("container");
		const renderer = new THREE.WebGLRenderer();
		const fov = 60;
		const camera = new THREE.PerspectiveCamera(fov, 1.0, 0.01, 1000);
		const scene = new THREE.Scene();
		scene.background = new THREE.Color(0xffffff);	
		
		renderer.shadowMap.enabled = true;
		renderer.setSize(400, 400);	
		el.appendChild(renderer.domElement);
		
		camera.position.set(0,2,25);
		scene.add(camera);
		
		
		// https://discourse.threejs.org/t/solved-glb-model-is-very-dark/6258
		// should always use a hemisphere light!
		var hemiLight = new THREE.HemisphereLight(0xffffff, 0x444444);
		hemiLight.position.set(0, 300, 0);
		scene.add(hemiLight);
		

		loadedModels.push(getModel('../static/basic_avatar_head.glb', 'avatar'));

		Promise.all(loadedModels).then((objects) => {
			objects.forEach((meshList) => {
				
				var count = 0;
				
				// note that i'm assuming a list of meshes coming in. change this later
				meshList.forEach((mesh) => {
					console.log(mesh);
					
					if(showWireframe){
						var wireframe = new THREE.WireframeGeometry(mesh.geometry);
						var line = new THREE.LineSegments(wireframe);
						line.material.depthTest = false;
						line.material.opacity = .8;
						line.material.transparent = true;
						group.add(line);//mesh);
					}else{
						group.add(mesh);
					}
				
					bgAxesHelper = new THREE.AxesHelper(10);
					group.add(bgAxesHelper);
					//mesh.position.set(0,0,0);
					
					let theMesh = showWireframe ? line : mesh;
					let meshGeometry = showWireframe ? line.geometry : mesh.geometry;
					
					if(mesh.name === "mesh0"){
						// the head 
						avatarParts.head = meshGeometry;
					}
					
					// loading the meshes is not always in the same order!!!
					if(mesh.name === "mesh1"){
						// right eye
						theMesh.position.set(-3.5,3,0);
						avatarParts.rightEye = meshGeometry;
					}
					
					if(mesh.name === "mesh2"){
						// left eye
						theMesh.position.set(3.5,3,0);
						avatarParts.leftEye = meshGeometry;
					}
					
					if(mesh.name === "mesh3"){
						// left eyebrow
						theMesh.position.set(3.5,6.5,0);
						avatarParts.leftEyebrow = meshGeometry;
					}
					
					if(mesh.name === "mesh4"){
						// right eyebrow
						theMesh.position.set(-3.5,6.5,0);
						avatarParts.rightEyebrow = meshGeometry;
					}
					
					if(mesh.name === "mesh5"){
						// mouth
						theMesh.position.set(0,-5,0);
						avatarParts.mouth = meshGeometry;
					}
					
					count++;
					
					if(count === 6){
						//mesh.castShadow = true;
						console.log(avatarParts);
						group.position.set(0,0,0);
						avatar = group;
					
						scene.add(group);

						renderer.render(scene, camera);
						meshLoaded = true;
					}
				});
			});
		});
			
	
	///////////////////////////////////////////////////////////
	
		let movingForwards = false;
		let movingBackwards = false;
		let zRotateLeft = false;
		let zRotateRight = false;
		let yRotateLeft = false;
		let yRotateRight = false;
		//let closedMouthBaseline = 0.0; // the DISTANCE between a point from the top lip and a point from the bottom lip to determine when a mouth should be closed or opened (by default should be considered closed)
										// to be determined by calibration
		
		let lastBottomLipPos = null;
		
		// handle receiving facial landmark coordinate data 
		socket.on('landmarkCoordinates', (data) => {
			let landmark_data = JSON.parse(data);
			
			if(meshLoaded && landmark_data.length > 0){
			
			
				// make sure we know what parts of landmark_data map to what facial landmarks 
				let leftEyeLandmark = landmark_data.slice(42, 48);
				let rightEyeLandmark = landmark_data.slice(36, 42);
				let leftEyebrowLandmark = landmark_data.slice(22, 26);
				let rightEyebrowLandmark = landmark_data.slice(17, 22);
				let jawLandmark = landmark_data.slice(0, 16);
				//let mouth = [landmark_data[63], landmark_data[67]];
				
				let bottomMouthVerts = [5,6,11,20,21,22,24,26,27,29,30,31,32,33,35,36,37,38];
				let cornerMouthVerts = [7,9,10,14,18];
			
				////////// start movement calculation stuff (i.e. determine how to rotate/translate head)
				if(calibrate){
					//calibrateAnchor(landmark_data[27], landmark_data[30]);
					calibrate = !calibrate;
					prevAnchor = landmark_data[27];
					
					
					// this is for the 3d model. not godd that we have to hunt for the right verts to ref but maybe we can fix that.
					// wait this might not even be necessary... the info from the 2d face tracking dontrols any movement that occurs on the model.
					// so, we only need to worry about the 2d side of things really (I think). i'm very confused right now lol.
					let mouthVerts = avatarParts.mouth.attributes.position.array; //[idx*3 + 1]; // +1 for the y coord
					let topMouthVert = {'x': mouthVerts[0], 'y': mouthVerts[1]}; // remember that mouthVerts is an array with each coordinate's x,y,z being their own index.
					let bottomMouthVert = {'x': mouthVerts[11*3], 'y': mouthVerts[11*3 + 1]};
					
					console.log("bottom mouth vert:");
					console.log(landmark_data[67]);
					console.log({'x': mouthVerts[11*3], 'y': mouthVerts[11*3 + 1]});
					console.log(avatarParts.mouth.attributes.position.array);
					
					console.log("top mouth vert:");
					console.log(landmark_data[63]);
					console.log({'x': mouthVerts[0], 'y': mouthVerts[1]});
					//console.log(avatarParts.mouth.attributes.position.array);
					
					let leftMouthVert = {'x': mouthVerts[14*3], 'y': mouthVerts[14*3 + 1]};
					let rightMouthVert = {'x': mouthVerts[7*3], 'y': mouthVerts[7*3 + 1]};
					
					calibrateMouthBaseline(landmark_data[63], landmark_data[67], landmark_data[48], landmark_data[54], topMouthVert, bottomMouthVert, leftMouthVert, rightMouthVert);
				}
			
				rightJawEnd = landmark_data[0];
				leftJawEnd = landmark_data[16];
				if(lastJawEndptDist === null){
					lastJawEndptDist = getDistance(leftJawEnd, rightJawEnd);
				}else{
					// note! moving the head up and down can trigger similar changes to moving backwards/forwards :<
					let currDist = getDistance(leftJawEnd, rightJawEnd);
					
					// how much room can we give before something is actually considered moving towards or away from the cam?
					// we should allow for some small dist changes before translating the avatar forwards or backwards
					if(currDist < lastJawEndptDist - 3){
						// moving backwards / away from camera 
						// by how much?
						movingBackwards = true;
						//document.getElementById('currentAction').textContent = "moving backwards...";
					}else if(currDist > lastJawEndptDist + 3){
						// moving forwards / towards camera
						//document.getElementById('currentAction').textContent = "moving forwards...";
						movingForwards = true;
					}
					
					//document.getElementById('currentJawDist').textContent = "curr jaw endpoints distance: " + currDist;
					
					// reset
					lastJawEndptDist = currDist;
				}
				
				
				// is the mouth expanding/closing?
				// get the change in distance from baseline
				if(lastBottomLipPos === null){
					lastBottomLipPos = landmark_data[67];
				}
				
				// I want to clamp angles so it shouldn't be possible to have the bottom lip y be higher than the top lip y 
				// also, rotations preserve distances so it should be ok to use the same baseline no matter the rotation. 
				// more notes:
				/*
					I think one of the problems is that I'm thinking by using just the middle mouth top/bottom vertices 
					I can control the movement of the mouth. but the other vertices for the bottom of the mouth, when 
					adding/subtracting distance to their y-coords, don't seem to stay in sync. 
					
					another issue is that when the model is imported with three.js, some vertices get duplicated because 
					it's triangulating everything. so this makes it a bit trickier to move stuff based on vertices 
					
					also, maybe we can define some boundaries:
					i.e. when top and bottom are within 0.5 or something distance-wise, don't allow further movement 
					when top and bottom are x distance-wise, that's wide enough and don't allow further expansion
					
					so we check distance first. forget the calibration stuff for now. if we do that later, I'm thinking 
					we have multiple calibration steps to figure out the boundaries. 
					
					also, we really should try clamping angles first as well.
				
				*/
				if(baselineMouthCalibration.dist2d){// && Math.abs(landmark_data[67].y - lastBottomLipPos.y) > 1.8){
				
					document.getElementById('mouthCheck').textContent = "closed mouth baseline ok. baseline distance 2d value: " + baselineMouthCalibration.dist2d;
				
					let currDist = getDistance(landmark_data[63], landmark_data[67]);
					let pastBaseline = landmark_data[67].y < landmark_data[63].y;
					
					// maybe don't compare with last pos but instead compare with just the baseline and set vertex pos 
					// that way (no addition of delta)
					let deltaDist = landmark_data[67].y - lastBottomLipPos.y; //currDist - baselineMouthCalibration.dist;
					
					// hey wait here's a new idea: can't we get a general vector of the angle the face is at? we can use that to figure out new positions of vertices if needed maybe.
					// try to make it more resilient to small fluctuations in y-axis of the current bottom mouth point by making sure currDist is large enough first
					if(!pastBaseline && Math.abs(deltaDist) >= 2.0 && currDist >= baselineMouthCalibration.dist2d){// && landmark_data[67].y > landmark_data[63].y){
					
						let amountToMove = (deltaDist*-1)/2; // invert deltaDist because in the 2d canvas world going up means subtracting (since top left corner is 0,0)
						
						bottomMouthVerts.forEach((idx) => {
							//document.getElementById('mouthCheck').textContent = "moving bottom mouth !";
							let vertY = avatarParts.mouth.attributes.position.array[idx*3 + 1]; // +1 for the y coord
							avatarParts.mouth.attributes.position.array[idx*3 + 1] = vertY + amountToMove;
						});
						
						lastBottomLipPos = landmark_data[67];
						
					}

				}
				
				// work on moving the ends of the mouth along the x axis as well!
				if(baselineMouthCalibration.left2d){
					cornerMouthVerts.forEach((idx) => {
						let v = idx*3;
						//let currVal = avatarParts.mouth.attributes.position.array[v];

						if(idx === 14 || idx === 18 || idx === 10){
							// left corner
							if(landmark_data[48].x <= baselineMouthCalibration.left2d.x && landmark_data[48].x >= baselineMouthCalibration.left2d.x - 10.0){
								//if(Math.abs((landmark_data[48].x - baselineMouthCalibration.left2d.x))/2 >= 1.0){
									let newPos = baselineMouthCalibration.left3d.x + (landmark_data[48].x - baselineMouthCalibration.left2d.x);
									avatarParts.mouth.attributes.position.array[v] = newPos;
									document.getElementById('mouthCheck').textContent = "left mouth corner x = baseline: " + baselineMouthCalibration.left3d.x  + ", 3d: " + newPos + ", 2d: " + landmark_data[48].x;
								//}
							}
						}else{
							// right corner
							if(landmark_data[54].x >= baselineMouthCalibration.right2d.x && landmark_data[54].x <= baselineMouthCalibration.right2d.x + 10.0){
								//if(Math.abs((landmark_data[54].x - baselineMouthCalibration.right2d.x))/2 >= 1.0){
									let newPos = baselineMouthCalibration.right3d.x + (landmark_data[54].x - baselineMouthCalibration.right2d.x);
									avatarParts.mouth.attributes.position.array[v] = newPos; //currVal + 0.5;
									document.getElementById('mouthCheck2').textContent = "right mouth corner x = baseline: " + baselineMouthCalibration.right3d.x  + ", 3d: " + newPos + ", 2d: " + landmark_data[54].x;
								//}
							}
						}
					});
				}
				
				
				// are we rotating the head about the z-axis (the axis coming at the camera), i.e. head tilts sideways
				if(prevAnchor){
				
					currAnchor = landmark_data[27]; // we just need one point?

					//let currAnchor2 = landmark_data[30];
					let angle = getAngle(currAnchor, prevAnchor);
					//document.getElementById('zAxisRotation2').textContent = "z-axis angle: " + angle + " degrees. currAnchor1.y: " + currAnchor1.y + ", anchorPoint.y: " + anchorPoint.y;
					if(Math.abs(angle) > 2 && !(inRange(leftJawEnd.y, rightJawEnd.y, 3.0))){
						//document.getElementById('zAxisRotation').textContent = "rotate about z-axis! angle: " + angle + " degrees.";
						updateAnchorPoint = true;
						if(angle > 0){
							zRotateRight = true;
						}else{
							zRotateLeft = true; 
						}
					}
				}
				
				// are we rotating about the y-axis (axis going up/down). captures head rotation when looking left/right
				// look at distance between nose point and jaw endpoints
				// uh but what if the head is tilted??? we can use the jaw endpoints to help (look at their y-values?)
				if(prevAnchor){
				
					currAnchor = landmark_data[27]; // we just need one point?
					
					// need to make sure the jaw endpoints are pretty much at the same level 
					// and check to see that the curr anchor pt. is 
					let diff = currAnchor.x - prevAnchor.x;
					let turnDirection = diff > 0.0 ? "left" : "right";
					
					if(inRange(leftJawEnd.y, rightJawEnd.y, 1.0) && !(inRange(currAnchor.x, prevAnchor.x, 2.0))){
						// if the curr pos of currAnchor1 is more than a certain amount away from the prev anchorPoint along the x-axis
						// and as long as the y-coords of the jaw endpoints are pretty similar
						updateAnchorPoint = true;
						//document.getElementById('yAxisRotation').textContent = "rotate about y-axis! direction: " + turnDirection + ", currAnchor.x: " + currAnchor.x + ", prevAnchor.x: " + prevAnchor.x;
						if(turnDirection === "left"){
							yRotateLeft = true;
						}else{
							yRotateRight = true;
						}
					}
					
				}
				
				
				// are we rotating about the x-axis (axis going left/right). captures nodding motions.
				// can use nose anchor point and just check y-axis?
				
				if(updateAnchorPoint){
					prevAnchor = currAnchor;
					updateAnchorPoint = false;
				}
				
				
				// are we translating the head? i.e. moving the head left/right without any rotations?
				// just sample a few points and get the diff?
				
				/////////// end movement calculation stuff
				
			
				let canvas = document.getElementById("display");
				let context = canvas.getContext("2d");
				
				// clear the canvas 
				context.clearRect(0, 0, 400, 400);
				
				// draw on the canvas 
				landmark_data.forEach((coord) => {
					context.fillRect(coord.x, coord.y, 2, 2);
				});
				
				// connect the dots
				// fortunately, the coords should be organized so it's easy to connect the parts
				// see: https://www.pyimagesearch.com/2017/04/10/detect-eyes-nose-lips-jaw-dlib-opencv-python/
				let coordsToSkip = new Set([16, 21, 26, 35, 41, 47, 67]); // 7 regions === 7 lines to draw
				for(let i = 0; i <= landmark_data.length-1; i++){
					// do not connect the last coord for each facial region (i.e. mouth, node, jaw, etc.) to anything
					
					// fix mouth (special case) 
					if(i === 67){
						// needs to connect with 60
						context.beginPath();
						context.moveTo(landmark_data[i].x, landmark_data[i].y);
						context.lineTo(landmark_data[60].x, landmark_data[60].y);
						context.stroke();
					}else if(coordsToSkip.has(i)){
						continue;
					}else{
						let x = landmark_data[i].x;
						let y = landmark_data[i].y;
						let x2 = landmark_data[i+1].x;
						let y2 = landmark_data[i+1].y;
						context.beginPath();
						context.moveTo(x, y);
						context.lineTo(x2, y2);
						context.stroke();
					}
				}
				
				
				// update mesh
				if(movingForwards){
					// just update camera!
					camera.translateZ(-1.0);
					movingForwards = false;
				}else if(movingBackwards){
					movingBackwards = false;
					camera.translateZ(1.0);
				}
				
				// rotate about z axis
				if(zRotateLeft){
					if(group.rotation.z < (Math.PI / 3)){
						group.rotateOnWorldAxis(new THREE.Vector3(0,0,1), Math.PI / 8);
					}
					zRotateLeft = false;
				}else if(zRotateRight){
					if(group.rotation.z > (-Math.PI / 3)){
						group.rotateOnWorldAxis(new THREE.Vector3(0,0,1), -Math.PI / 8);
					}
					zRotateRight = false;
				}
				
				/*
				// rotate about y axis
				if(yRotateLeft){
					if(group.rotation.y < (Math.PI / 3)){
						// clamp the rotation
						group.rotateOnWorldAxis(new THREE.Vector3(0,1,0), Math.PI / 8);
					}
					yRotateLeft = false;
				}else if(yRotateRight){
					if(group.rotation.y > (-Math.PI / 3)){
						group.rotateOnWorldAxis(new THREE.Vector3(0,1,0), -Math.PI / 8);
					}
					yRotateRight = false;
				}*/
				
				
				for(let part in avatarParts){
					avatarParts[part].attributes.position.needsUpdate = true;
				}
				renderer.render(scene, camera);
				
			}
			//console.log(landmark_data);
			//console.log("==========================")
		});
		
	</script>



</html>