<html>

	<head>
		<title>vstream display</title>
		<script src="//cdnjs.cloudflare.com/ajax/libs/socket.io/2.2.0/socket.io.js" integrity="sha256-yr4fRk/GU1ehYJPAs8P4JlTgu0Hdsp4ZKrx8bDEDC3I=" crossorigin="anonymous"></script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/three.js/108/three.min.js'></script>
		<script src='static/GLTFLoader.js'></script>
	</head>
	
	<style>
		#main{
			font-family: monospace;
			text-align: center;
			top: 5%;
			position: relative;
			display: block;
		}
		canvas{
			border: #000 solid 1px;
		}
		#container{
			width: 400px;
			height: 400px;
			margin: 0 auto;
			display: inline-block;
		}
	</style>
	
	<body>
		<div id='main'>
			<h3> vstream display </h3>
			
			<div id='show'>
				<canvas id='display' width='400px' height='400px'>
				</canvas>
			
				<div id='container'>
				</div>
			</div>
			
			<br />
			<br />
			
			<button id='calibrate'> calibrate </button>
			<button id='toggleStream'> pause/continue </button>
			<button id='showLandmarkCoords'>show landmark coordinates</button>
			<button id='testMode'>test mode</button>
			<!--<button id='toggleWireframe'> toggle wireframe </button>-->
			<br />
			
			<div>
				<h3> current landmark coords: </h3>
				<pre id='jsonLandmarkCoords'>
				</pre>
			</div>
			
			
		</div>
	</body>


	<script>	
		const socket = io();
		let streamPaused = false;
		let calibrate = false;
		let showWireframe = false;
		let showLandmarkCoords = false;
		let testMode = false;
		
		// maybe instead of anchor points, we just save all the points of the previous frame to compare with. :>
		// let's use landmark coordinate number 27 and 30 (that's part of the nose)
		let lastJawEndptDist = null;
		let rightJawEnd = null;
		let leftJawEnd = null;
		let updateAnchorPoint = false;
		let prevAnchor = null;
		let currAnchor = null;
		
		const mapping = {}; // mapping important meshes to some info (i.e. baseline info)
		
		function toggleStream(){
			streamPaused = !streamPaused;
			socket.emit("toggleStream", streamPaused);
		}
		
		document.getElementById("toggleStream").addEventListener("click", (evt) => {
			console.log("toggling stream...");
			toggleStream();
		});
		
		document.getElementById("showLandmarkCoords").addEventListener("click", (evt) => {
			showLandmarkCoords = !showLandmarkCoords;
		});
		

		function getDistance(point1, point2){
			let xDiff = point2.x - point1.x;
			let yDiff = point2.y - point1.y;
			return Math.sqrt((xDiff * xDiff) + (yDiff * yDiff));
		}
		
		// pass in 2 coordinates
		// https://stackoverflow.com/questions/14066933/direct-way-of-computing-clockwise-angle-between-2-vectors
		function getAngle(vec1, vec2){
			let dot = (vec1.x * vec2.x) + (vec1.y * vec2.y);
			let det = (vec1.x * vec2.y) - (vec1.y * vec2.x);
			let angle = Math.atan2(det, dot) * (180 / Math.PI);
			return angle;
		}
		
		// pass in x and y coords from 2d space
		// because of how 2d canvas coordinates are (0,0 being the top left corner, no neg. coords),
		// we need to convert so we can know where a 2d coord can get mapped to in 3d space
		function convert2dCoords(x, y, width, height){
			let newCoords = {};
			if(x > width/2){
				newCoords.x = x;
			}
		}
		
		function inRange(x1, x2, rangeLimit){
			return (x1 <= x2 + rangeLimit) && (x1 >= x2 - rangeLimit);
		}
		
		function clamp(val, min, max){
			val = val > max ? max : val;
			val = val < min ? min : val;
			return val;
		}
		
		// create a function that gets the interpolated y-value given an x for a specific 
		// range (given by x1,y1 and x2,y2)
		// https://en.wikipedia.org/wiki/Interpolation
		// this is useful for determining morph influence on certain 3d model targets like the mouth (when using shape keys)
		function createInterpolation(x1, y1, x2, y2){
			return function(newX){
				return y1 + (newX - x1)*((y2-y1)/(x2-x1));
			}
		}
		
		// set the initial nose/anchor point that we can use to figure out when a rotation is happening
		function calibrateBaseline(landmarkData, pupilData, map){
			// map points from landmark coords to face meshes
			// this mapping thing should be configurable via a json file the user can import? 
			// 'cause models can be different
			// stuff like interpolation parameters should be configurable too 
			for(let part in avatarParts){
				if(part === "mouth"){
					// get the ratio of distance between vertices of the inner part of the lips and the outer part (for determining vertical movement)
					// in doing so we can try to get the correct mouth movement no matter how close or far the user's face is to the camera
					let topInner = landmarkData[62];
					let bottomInner = landmarkData[66];
					let vertDistInner = getDistance(topInner, bottomInner);

					let topOuter = landmarkData[51];
					let bottomOuter = landmarkData[57];
					let vertDistOuter = getDistance(topOuter, bottomOuter);
					
					let vertDistRatio = vertDistInner / vertDistOuter; // we're assuming at baseline the mouth is closed! so at this ratio, morph influence is 0.0
					console.log("setting vertDistRatio: " + vertDistRatio);

					// for horizontal movement, the mouth corner vertices will generally stay in the same place. so to have some useful distance that 
					// we can use to help us figure out which way the mouth should stretch, no matter how close or far the user's face is from the camera,
					// we can use the uppermost jaw landmark coordinates
					let leftJawUpper = landmarkData[0];
					let rightJawUpper = landmarkData[16];
					let leftMouthCornerOuter = landmarkData[48];
					let rightMouthCornerOuter = landmarkData[54];
					let horzDistOuter = getDistance(leftMouthCornerOuter, rightMouthCornerOuter);
					let jawDistUpper = getDistance(leftJawUpper, rightJawUpper);
					
					let horzDistRatio = horzDistOuter / jawDistUpper;
					console.log("setting horzDistRatio: " + horzDistRatio);
					
					map['mouth'] = {
						'vertical': {
							// note that the parameters here can determine the 'sensitivity' of the morph (i.e. vertDist*3 describes the distance at which the morph influence is maximum)
							// note: these max/min values don't matter too much since we clamp the resulting value anyway
							'interpolate': createInterpolation(vertDistRatio, 0.0, vertDistRatio*4, 1.0)
						},
						'horizontal': {
							// is there a way to get the min range value from the gltf?
							'interpolate': createInterpolation(horzDistRatio-0.2, -1.0, horzDistRatio+0.2, 1.0) // as the ratio gets smaller (distance between outer mouth vertices increases), the mouth should widen
						}
					}
				}
				
				// the actual 'left' eye when you're facing the screen
				if(part === "leftEye"){
					let top = landmarkData[44];
					let right = landmarkData[45];
					let pupil = pupilData[1];
					let horzRightDist = getDistance(pupil, right);
					let vertTopDist = getDistance(pupil, top);
					
					let bottom = landmarkData[46];
					let left = landmarkData[42]; 
					let horzLeftDist = getDistance(pupil, left);
					let vertBottomDist = getDistance(pupil, bottom);
					
					map['leftEye'] = {
						'horizontal': {
							'left': {
								'interpolate': createInterpolation(horzLeftDist, 0.0, horzLeftDist-1, 1.0) // but note this is going from the center of the eye socket towards the middle of the face. (moveleft shape key)
							},
							'right': {
								'interpolate': createInterpolation(horzRightDist, 0.0, horzRightDist-1, 1.0)
							}
						},
						'vertical': {
							'top': {
								'interpolate': createInterpolation(vertTopDist, 0.0, vertTopDist-1, 1.0)
							},
							'bottom': {
								'interpolate': createInterpolation(vertBottomDist, 0.0, vertBottomDist-1, 1.0)
							}
						},
					}
				}
				
				if(part === "rightEye"){
					let top = landmarkData[38];
					let right = landmarkData[39]; // the right-most landmark of the left eye
					let pupil = pupilData[0];
					let horzRightDist = getDistance(pupil, right);
					let vertTopDist = getDistance(pupil, top);
					
					let bottom = landmarkData[40];
					let left = landmarkData[36]; 
					let horzLeftDist = getDistance(pupil, left);
					let vertBottomDist = getDistance(pupil, bottom);
					
					map['rightEye'] = {
						'horizontal': {
							'left': {
								'interpolate': createInterpolation(horzLeftDist, 0.0, horzLeftDist-1, 1.0)
							},
							'right': {
								'interpolate': createInterpolation(horzRightDist, 0.0, horzRightDist-1, 1.0)
							}
						},
						'vertical': {
							'top': {
								'interpolate': createInterpolation(vertTopDist, 0.0, vertTopDist-1, 1.0) // decreasing vertBottomDist means the pupil is getting closer to bottom of the eye socket
							},
							'bottom': {
								'interpolate': createInterpolation(vertBottomDist, 0.0, vertBottomDist-1, 1.0)
							}
						},
					}
				}
				
				if(part === "leftEyebrow"){
					// the eyebrow will be treated a little differently in that 
					// I think using the rightmost coord of the left eyebrow (left when facing the screen,
					// so the coord closest to the top of the line going down the nose)
					// and the leftmost coord of the right eyebrow and comparing distance between the coord
					// at the top of the nose can help
					let middle = landmarkData[27];
					let left = landmarkData[21];
					let dist = getDistance(left, middle);
					map['leftEyebrow'] = {
						'dist': {
							'baseline': dist,
							'previous': 0
						},
						'interpolate': createInterpolation(dist, 0.0, dist/2, 1.0) // this causes an inverse relationship between X and Y!
					}
				}
				
				if(part === "rightEyebrow"){
					let middle = landmarkData[27];
					let right = landmarkData[22];
					let dist = getDistance(right, middle);
					map['rightEyebrow'] = {
						'dist': {
							'baseline': dist,
							'previous': 0
						},
						'interpolate': createInterpolation(dist, 0.0, dist/2, 1.0)
					}
				}
				
				if(part === "head"){
					// control blinking
					
					//handle left eye blink
					// remember, going towards a morph influence value of 1.0 (full strength) means the eye is being closed
					// 0.0 => eyes opened
					let topLeft = landmarkData[38];
					let bottomLeft = landmarkData[40];
					let distLeft = getDistance(topLeft, bottomLeft);
					map['leftEyeBlink'] = {
						'dist': {
							'baseline': distLeft,
							'previous': 0
						},
						'interpolate': createInterpolation(distLeft, 0.0, distLeft/2, 1.0)
					}
					
					//handle right eye blink
					let topRight = landmarkData[44];
					let bottomRight = landmarkData[46];
					let distRight = getDistance(topRight, bottomRight);
					map['rightEyeBlink'] = {
						'dist': {
							'baseline': distRight,
							'previous': 0
						},
						'interpolate': createInterpolation(distRight, 0.0, distRight/2, 1.0)
					}
				}
				
			}

		}
		
		document.getElementById('calibrate').addEventListener('click', (evt) => {
			calibrate = !calibrate;
		});
		
		
		
		//////////////////////////////////////////// three js stuff
		function getModel(modelFilePath, name){
			console.log(modelFilePath);
			return new Promise((resolve, reject) => {
				loader.load(
					modelFilePath,
					function(gltf){
						if(gltf.animations.length > 0){
							//console.log(gltf.animations);
						}
						let seen = new Set();
						let faceParts = [];
						gltf.scene.traverse((child) => {
							if(child.type === "Mesh"){
								let material = child.material;
								let geometry = child.geometry;
								let obj = new THREE.Mesh(geometry, material);
								
								// https://stackoverflow.com/questions/52569738/how-to-access-single-vertices-of-mesh-loaded-with-gltfloader-in-three-js
								if(child.parent.name !== "Scene"){
									if(!seen.has(child.parent.name)){
										// this child is part of a group
										child.parent.scale.x = child.parent.scale.x * 5;
										child.parent.scale.y = child.parent.scale.y * 5;
										child.parent.scale.z = child.parent.scale.z * 5;
										faceParts.push(child.parent);
										seen.add(child.parent.name);
									}
								}else{
									//console.log(child.name);
									obj.scale.x = child.scale.x * 5;
									obj.scale.y = child.scale.y * 5;
									obj.scale.z = child.scale.z * 5;
									obj.name = child.name;
									faceParts.push(obj);
								}
								
							}
						});
						resolve(faceParts);
					},
					// called while loading is progressing
					function(xhr){
						console.log( (xhr.loaded / xhr.total * 100) + '% loaded' );
					},
					// called when loading has errors
					function(error){
						console.log('An error happened');
						console.log(error);
						console.trace();
					}
				);
			});
		}
		
		const loader = new THREE.GLTFLoader();
		let loadedModels = [];
		
		const avatarParts = {
			"head": null,
			"leftEyebrow": null,
			"leftEye": null,
			"rightEyebrow": null,
			"rightEye": null,
			"mouth": null
		};
		
		const group = new THREE.Group();
		let meshLoaded = false;
		
		const el = document.getElementById("container");
		const renderer = new THREE.WebGLRenderer();
		const fov = 60;
		const camera = new THREE.PerspectiveCamera(fov, 1.0, 0.01, 1000);
		const scene = new THREE.Scene();
		scene.background = new THREE.Color(0xffffff);	
		
		renderer.shadowMap.enabled = true;
		renderer.setSize(400, 400);	
		el.appendChild(renderer.domElement);
		
		camera.position.set(0,2,25);
		scene.add(camera);
		
		// https://discourse.threejs.org/t/solved-glb-model-is-very-dark/6258
		var hemiLight = new THREE.HemisphereLight(0xffffff, 0x444444);
		hemiLight.position.set(0, 200, 0);
		scene.add(hemiLight);
	
		var dirLight = new THREE.DirectionalLight( 0xffffff );
		dirLight.position.set( 0, 100, -10);
		scene.add( dirLight );
		
		// basic_avatar_head-edit3.gltf is experimenting with a different kind of mesh setup for the eye.
		loadedModels.push(getModel('../static/basic_avatar_head-edit5-2.gltf', 'avatar'));

		Promise.all(loadedModels).then((objects) => {
			objects.forEach((meshList) => {
				
				var count = 0;
				
				// note that i'm assuming a list of meshes coming in. change this later
				meshList.forEach((mesh) => {
					
					if(showWireframe){
						var wireframe = new THREE.WireframeGeometry(mesh.geometry);
						var line = new THREE.LineSegments(wireframe);
						line.material.depthTest = false;
						line.material.opacity = .8;
						line.material.transparent = true;
						group.add(line);
					}else{
						group.add(mesh);
					}
				
					bgAxesHelper = new THREE.AxesHelper(10);
					group.add(bgAxesHelper);
					
					let theMesh = showWireframe ? line : mesh;
					let meshGeometry = showWireframe ? line.geometry : mesh;  // kinda misleading...
					
					if(mesh.name === "head"){
						// the head 
						// note that for the current avatar, the head mesh controls blinking!!
						avatarParts.head = meshGeometry;
					}
					
					// loading the meshes is not always in the same order!!!
					if(mesh.name === "rightEye"){
						// right eye
						theMesh.position.set(-3.5,2,0);
						avatarParts.rightEye = meshGeometry; 
					}
					
					if(mesh.name === "leftEye"){
						// left eye
						theMesh.position.set(3.5,2,0);
						avatarParts.leftEye = meshGeometry;
					}
					
					if(mesh.name === "leftEyebrow"){
						// left eyebrow
						theMesh.position.set(3.5,4.5,0);
						avatarParts.leftEyebrow = meshGeometry;
					}
					
					if(mesh.name === "rightEyebrow"){
						// right eyebrow
						theMesh.position.set(-3.5,4.5,0);
						avatarParts.rightEyebrow = meshGeometry;
					}
					
					if(mesh.name === "mouth"){
						// mouth
						theMesh.position.set(0,-5,0);
						avatarParts.mouth = meshGeometry;
					}
					
					count++;
					
					if(count === 6){
						console.log(avatarParts);
						group.position.set(0,0,0);
						avatar = group;
					
						scene.add(group);

						renderer.render(scene, camera);
						meshLoaded = true;
					}
				});
			});
		});
		
		function handleMouthMovement(mapping, landmark_data, avatarParts){
			let mouthVertical = mapping['mouth'].vertical;
			let mouthHorizontal = mapping['mouth'].horizontal;

			let innerMouthTop = landmark_data[62];
			let innerMouthBottom = landmark_data[66];
			let outerMouthTop = landmark_data[51];
			let outerMouthBottom = landmark_data[57];
			let currVertRatio = getDistance(innerMouthTop, innerMouthBottom) / getDistance(outerMouthTop, outerMouthBottom);
			let newValVert = mapping.mouth.vertical.interpolate(currVertRatio);
			avatarParts.mouth.morphTargetInfluences[0] = clamp(newValVert, 0, 1.0);

			let rightJawUpper = landmark_data[16];
			let leftJawUpper = landmark_data[0];
			let leftMouthOuter = landmark_data[48];
			let rightMouthOuter = landmark_data[54];
			let currHorzRatio = getDistance(leftMouthOuter, rightMouthOuter) / getDistance(leftJawUpper, rightJawUpper);
			let newValHorz = mapping.mouth.horizontal.interpolate(currHorzRatio); // remember we get a interpolated value within the arbitrary range we set earlier
			avatarParts.mouth.morphTargetInfluences[2] = clamp(newValHorz, 0, 0.8); // I set the max value for the shape key in Blender to 0.8
			
			// use newValHorz and newValVert to determine the influence on the o-shape mouth shape key
			// TODO: do not use newValVert but make a new interpolation for the top lip center vertices?
			if(newValHorz < -0.3){
				// the newValHorz being less than 0 is super important. when your mouth forms an o-shape, I think generally the corners come closer together.
				avatarParts.mouth.morphTargetInfluences[1] = clamp(Math.abs(newValHorz), 0, 0.4);
			}else{
				avatarParts.mouth.morphTargetInfluences[1] = 0;
			}
		}
		
		function handleEyeMovement(mapping, landmark_data, avatarParts){
			// this is actually controlling blinking! the head mesh has the morph targets for this.
			// but we can store info about the eye coords in their respective mappings but just make sure to 
			// affect the head's morph targets
			
			// ok so I made this a bit confusing, sorry -__-
			// the shape key is closeEye so the larger the morph influence value, the more closed the eye gets.
			// so 1.0 == closed eye, 0.0 == open eye.
			
			let leftEyeVertical = mapping['leftEyeBlink'].dist;
			let rightEyeVertical = mapping['rightEyeBlink'].dist;
			let lastVertLeft, lastVertRight;
			let minDelta = 1.4;
			
			lastVertLeft = leftEyeVertical.previous === 0 ? leftEyeVertical.baseline : leftEyeVertical.previous;
			lastVertRight = rightEyeVertical.previous === 0 ? rightEyeVertical.baseline : rightEyeVertical.previous;

			let currDistLeft = getDistance(landmark_data[38], landmark_data[40]);
			let currDistRight = getDistance(landmark_data[44], landmark_data[46]);
			
			let deltaVertLeft = (currDistLeft - lastVertLeft);
			if(Math.abs(deltaVertLeft) > minDelta){		
				let newVal = mapping.leftEyeBlink.interpolate(currDistLeft);		
				avatarParts.head.morphTargetInfluences[0] = clamp(newVal, 0, 1); // left eye
				mapping['leftEyeBlink'].dist.previous = currDistLeft;
			}
			
			let deltaVertRight = (currDistRight - lastVertRight);
			if(Math.abs(deltaVertRight) > minDelta){
				let newVal = mapping.rightEyeBlink.interpolate(currDistRight);
				avatarParts.head.morphTargetInfluences[1] = clamp(newVal, 0, 1);// right eye
				mapping['rightEyeBlink'].dist.previous = currDistRight;
			}
		}
		
		function handleEyebrowMovement(mapping, landmark_data, avatarParts){		
			let left = mapping.leftEyebrow.dist;
			let right = mapping.rightEyebrow.dist;
			let minDelta = 1.0;
			
			let leftPrev = left.previous === 0 ? left.baseline : left.previous;
			let rightPrev = right.previous === 0 ? right.baseline : right.previous;
			
			let currDistLeft = getDistance(landmark_data[21], landmark_data[27]);
			let currDistRight = getDistance(landmark_data[22], landmark_data[27]);
			
			// hmmmm https://github.com/davisking/dlib/issues/1658
			// forget the eyebrow raising for now
			let deltaLeft = currDistLeft - left.baseline;
			let deltaRight = currDistRight - right.baseline;
			
			// eyebrow down
			// note that I don't think we actually need to remember the previous distance amount. just using the baseline may be enough
			// with some interpolation. seems to work ok so far :)
			if(Math.abs(deltaLeft) > minDelta){
				// we need to move the eyebrow down
				let newVal = mapping.leftEyebrow.interpolate(currDistLeft);
				avatarParts.leftEyebrow.morphTargetInfluences[1] = clamp(newVal, 0, 1);
				mapping.leftEyebrow.dist.previous = currDistLeft;
			}
			
			if(Math.abs(deltaRight) > minDelta){
				// we need to move the eyebrow down
				let newVal = mapping.rightEyebrow.interpolate(currDistRight);
				avatarParts.rightEyebrow.morphTargetInfluences[1] = clamp(newVal, 0, 1);
				mapping.rightEyebrow.dist.previous = currDistRight;
			}
		}
		
		function handleLeftEyeball(mapping, pupil_data, landmark_data, avatarParts){
			let leftEyeTop = landmark_data[44];
			let leftEyeRight = landmark_data[45];
			let leftEyePupil = pupil_data[1] || {'x': 0, 'y': 0};
			let leftEyeBottom = landmark_data[46];
			let leftEyeLeft = landmark_data[42];

			let eyeLeftHorzRight = getDistance(leftEyePupil, leftEyeRight);
			let eyeLeftHorzLeft = getDistance(leftEyePupil, leftEyeLeft);
			
			let newHorzValRight = clamp(mapping.leftEye.horizontal.right.interpolate(eyeLeftHorzRight), 0.0, 1.0);
			let newHorzValLeft = clamp(mapping.leftEye.horizontal.left.interpolate(eyeLeftHorzLeft), 0.0, 1.0);
			let minHorz = Math.min(newHorzValRight, newHorzValLeft);
			
			if(minHorz === newHorzValRight){			
				avatarParts.leftEye.morphTargetInfluences[0] = 0.0;
				avatarParts.leftEye.morphTargetInfluences[1] = clamp(newHorzValRight, 0, 1);
			}else{
				avatarParts.leftEye.morphTargetInfluences[1] = 0.0;
				avatarParts.leftEye.morphTargetInfluences[0] = clamp(newHorzValLeft, 0 ,1);
			}
			
			// handle vertical movement
			let eyeLeftVertTop = getDistance(leftEyePupil, leftEyeTop);
			let eyeLeftVertBottom = getDistance(leftEyePupil, leftEyeBottom);			
			
			// whatever the min is between the top and bottom distance will tell us which direction the pupil is moving towards?
			let minVert = Math.min(eyeLeftVertTop, eyeLeftVertBottom);
			if(minVert === eyeLeftVertTop){
				avatarParts.leftEye.morphTargetInfluences[3] = 0.0;
				avatarParts.leftEye.morphTargetInfluences[2] = clamp(minVert, 0, 1);				
			}else{
				avatarParts.leftEye.morphTargetInfluences[2] = 0.0;
				avatarParts.leftEye.morphTargetInfluences[3] = clamp(minVert, 0, 1);
			}
		}
		
		function handleRightEyeball(mapping, pupil_data, landmark_data, avatarParts){
			let rightEyeTop = landmark_data[38];
			let rightEyeRight = landmark_data[39];
			let rightEyePupil = pupil_data[0] || {'x': 0, 'y': 0};
			let rightEyeBottom = landmark_data[40];
			let rightEyeLeft = landmark_data[36];
			
			// handle right eye	
			let eyeRightHorzRight = getDistance(rightEyePupil, rightEyeRight);
			let eyeRightHorzLeft = getDistance(rightEyePupil, rightEyeLeft);
		
			let newHorzValRight = clamp(mapping.rightEye.horizontal.right.interpolate(eyeRightHorzRight), 0.0, 1.0);
			let newHorzValLeft = clamp(mapping.rightEye.horizontal.left.interpolate(eyeRightHorzLeft), 0.0, 1.0);
			let minHorz = Math.min(newHorzValRight, newHorzValLeft);
			if(minHorz === newHorzValRight){
				// moving the pupil to the right				
				avatarParts.rightEye.morphTargetInfluences[0] = 0.0;
				avatarParts.rightEye.morphTargetInfluences[1] = clamp(newHorzValRight, 0, 1);
			}else{
				// moving the pupil to the left			
				avatarParts.rightEye.morphTargetInfluences[1] = 0.0;
				avatarParts.rightEye.morphTargetInfluences[0] = clamp(newHorzValLeft, 0, 1);
			}
			
			// handle vertical movement
			let eyeRightVertTop = getDistance(rightEyePupil, rightEyeTop);
			let eyeRightVertBottom = getDistance(rightEyePupil, rightEyeBottom);			
			
			// whatever the min is between the top and bottom distance will tell us which direction the pupil is moving towards?
			let minVert = Math.min(eyeRightVertTop, eyeRightVertBottom);
			if(minVert === eyeRightVertTop){
				avatarParts.rightEye.morphTargetInfluences[3] = 0.0;
				avatarParts.rightEye.morphTargetInfluences[2] = clamp(minVert, 0 , 1);				
			}else{
				avatarParts.rightEye.morphTargetInfluences[2] = 0.0;
				avatarParts.rightEye.morphTargetInfluences[3] = clamp(minVert, 0, 1);	
			}
			
			/*
			leaving this here because it was a neat-ish idea I think
			
			let baselineVec = {
				'x': rightEyeRight.x - mapping.rightEye.baselineCoords.x, 
				'y': rightEyeRight.y - mapping.rightEye.baselineCoords.y
			};
			
			// get current vector 
			let currVecRight = {
				'x': rightEyePupil.x - mapping.rightEye.baselineCoords.x, 
				'y': rightEyePupil.y - mapping.rightEye.baselineCoords.y
			};
			
			// cross product the vectors to know if the pupil is below or above the baseline vector
			let crossProdRight = (baselineVec.x*currVecRight.y) - (baselineVec.y*currVecRight.x);
			if(crossProdRight < 0){
				//console.log("moving pupil UP! crossProd is: " + crossProd);
				let newVertVal = clamp(mapping.rightEye.vertical.top.interpolate(rightEyePupil.y), 0.0, 1.0);
				avatarParts.rightEye.morphTargetInfluences[3] = 0.0;
				avatarParts.rightEye.morphTargetInfluences[2] = newVertVal;
			}else if(crossProdRight === 0){
				//console.log("no vertical movement for pupil! crossProd is: " + crossProd);
				avatarParts.rightEye.morphTargetInfluences[3] = 0.0;
				avatarParts.rightEye.morphTargetInfluences[2] = 0.0;
			}else{
				//console.log("moving pupil DOWN! crossProd is: " + crossProd);
				let newVertVal = clamp(mapping.rightEye.vertical.bottom.interpolate(rightEyePupil.y), 0.0, 1.0);
				avatarParts.rightEye.morphTargetInfluences[2] = 0.0;
				avatarParts.rightEye.morphTargetInfluences[3] = newVertVal;
			}*/
		}
		
		function handleEyeballMovement(mapping, landmark_data, pupil_data, avatarParts){
			// each eyeball can move up, down, left, right
			handleLeftEyeball(mapping, pupil_data, landmark_data, avatarParts);
			handleRightEyeball(mapping, pupil_data, landmark_data, avatarParts);
		}
	
		///////////////////////////////////////////////////////////
		
		function updateCanvas(landmark_data, pupil_coords, context){
			// clear the canvas 
			context.clearRect(0, 0, 400, 400);
			
			// draw on the canvas 
			landmark_data.forEach((coord) => {
				context.fillRect(coord.x, coord.y, 2, 2);
			});
			
			pupil_coords.forEach((coord) => {
				context.fillRect(coord.x, coord.y, 3, 3);
			});
			
			// connect the dots
			// fortunately, the coords should be organized so it's easy to connect the parts
			// see: https://www.pyimagesearch.com/2017/04/10/detect-eyes-nose-lips-jaw-dlib-opencv-python/
			let coordsToSkip = new Set([16, 21, 26, 35, 41, 47, 67]); // 7 regions === 7 lines to draw
			for(let i = 0; i <= landmark_data.length-1; i++){
				// do not connect the last coord for each facial region (i.e. mouth, node, jaw, etc.) to anything
				
				// fix mouth (special case) 
				if(i === 67){
					// needs to connect with 60
					context.beginPath();
					context.moveTo(landmark_data[i].x, landmark_data[i].y);
					context.lineTo(landmark_data[60].x, landmark_data[60].y);
					context.stroke();
				}else if(coordsToSkip.has(i)){
					continue;
				}else{
					let x = landmark_data[i].x;
					let y = landmark_data[i].y;
					let x2 = landmark_data[i+1].x;
					let y2 = landmark_data[i+1].y;
					context.beginPath();
					context.moveTo(x, y);
					context.lineTo(x2, y2);
					context.stroke();
				}
			}
		}
	
		let movingForwards = false;
		let movingBackwards = false;
		let zRotateLeft = false;
		let zRotateRight = false;
		let yRotateLeft = false;
		let yRotateRight = false;
		
		let prevCoordPositions = {};
		
		// handle receiving facial landmark coordinate data 
		socket.on('landmarkCoordinates', (data) => {
			let theData = JSON.parse(data);
			let landmark_data = theData['landmark_coords'];
			let pupil_coords = theData['pupil_coords'];
			
			if(meshLoaded && landmark_data.length > 0){
			
				if(showLandmarkCoords){
					document.getElementById('jsonLandmarkCoords').textContent = JSON.stringify(theData, null, 1);
				}else{
					document.getElementById('jsonLandmarkCoords').textContent = "";
				}
				
				////////// start movement calculation stuff (i.e. determine how to rotate/translate head)
				if(calibrate){
					console.log("calibrating...");
					calibrate = !calibrate;
					prevAnchor = landmark_data[27];
					calibrateBaseline(landmark_data, pupil_coords, mapping); // mapping defined above as empty dictionary
				}
				
				if(testMode){
					// stop stream, send landmark coords based on manual pose selection
					toggleStream();
					
					// show pose options to test
					
					return;
				}
			
				rightJawEnd = landmark_data[0];
				leftJawEnd = landmark_data[16];
				if(lastJawEndptDist === null){
					lastJawEndptDist = getDistance(leftJawEnd, rightJawEnd);
				}else{
					// note! moving the head up and down can trigger similar changes to moving backwards/forwards :<
					let currDist = getDistance(leftJawEnd, rightJawEnd);
					
					// how much room can we give before something is actually considered moving towards or away from the cam?
					// we should allow for some small dist changes before translating the avatar forwards or backwards
					if(currDist < lastJawEndptDist - 3){
						// moving backwards / away from camera 
						// by how much?
						movingBackwards = true;
						//document.getElementById('currentAction').textContent = "moving backwards...";
					}else if(currDist > lastJawEndptDist + 3){
						// moving forwards / towards camera
						//document.getElementById('currentAction').textContent = "moving forwards...";
						movingForwards = true;
					}
					
					// reset
					lastJawEndptDist = currDist;
				}
				

				if(mapping['mouth']){
					// now that we have shape keys for mouth movement, we only need to worry about how much we need to set the morphInfluence as given 
					// the current state of the facial landmarks 
					// but wait!! what about scaling? i.e. at calibration the face is at a certain distance from the camera, which affects the distance between 
					// points, right? wouldn't it be good to have some kind of scaling so we don't miscalculate?
					handleMouthMovement(mapping, landmark_data, avatarParts);
				}
				
				if(mapping['leftEye'] && mapping['rightEye']){
					handleEyeMovement(mapping, landmark_data, avatarParts);
					handleEyeballMovement(mapping, landmark_data, pupil_coords, avatarParts);
				}
				
				if(mapping['leftEyebrow'] && mapping['rightEyebrow']){
					handleEyebrowMovement(mapping, landmark_data, avatarParts);
				}
				
				
				// are we rotating the head about the z-axis (the axis coming at the camera), i.e. head tilts sideways
				if(prevAnchor){
					currAnchor = landmark_data[27]; // we just need one point?

					//let currAnchor2 = landmark_data[30];
					let angle = getAngle(currAnchor, prevAnchor);
					
					if(Math.abs(angle) > 2 && !(inRange(leftJawEnd.y, rightJawEnd.y, 3.0))){
						//document.getElementById('zAxisRotation').textContent = "rotate about z-axis! angle: " + angle + " degrees.";
						updateAnchorPoint = true;
						if(angle > 0){
							zRotateRight = true;
						}else{
							zRotateLeft = true; 
						}
					}
				}
				
				// are we rotating about the y-axis (axis going up/down). captures head rotation when looking left/right
				// look at distance between nose point and jaw endpoints
				// uh but what if the head is tilted??? we can use the jaw endpoints to help (look at their y-values?)
				if(prevAnchor){
					currAnchor = landmark_data[27]; // we just need one point?
					
					// need to make sure the jaw endpoints are pretty much at the same level 
					// and check to see that the curr anchor pt. is 
					let diff = currAnchor.x - prevAnchor.x;
					let turnDirection = diff > 0.0 ? "left" : "right";
					
					if(inRange(leftJawEnd.y, rightJawEnd.y, 1.0) && !(inRange(currAnchor.x, prevAnchor.x, 2.0))){
						// if the curr pos of currAnchor1 is more than a certain amount away from the prev anchorPoint along the x-axis
						// and as long as the y-coords of the jaw endpoints are pretty similar
						updateAnchorPoint = true;
						if(turnDirection === "left"){
							yRotateLeft = true;
						}else{
							yRotateRight = true;
						}
					}
				}
					
				// are we rotating about the x-axis (axis going left/right). captures nodding motions.
				// can use nose anchor point and just check y-axis?
				if(updateAnchorPoint){
					prevAnchor = currAnchor;
					updateAnchorPoint = false;
				}
					
				// are we translating the head? i.e. moving the head left/right without any rotations?
				// just sample a few points and get the diff?
				
				/////////// end movement calculation stuff
						
				// update canvas with landmark coordinates
				let canvas = document.getElementById("display");
				let context = canvas.getContext("2d");
				updateCanvas(landmark_data, pupil_coords, context);


				// move avatar closer or away from camera
				/*
				if(movingForwards){
					// just update camera!
					camera.translateZ(-1.0);
					movingForwards = false;
				}else if(movingBackwards){
					movingBackwards = false;
					camera.translateZ(1.0);
				}*/
				
				/* rotate about z axis
				if(zRotateLeft){
					if(group.rotation.z < (Math.PI / 3)){
						group.rotateOnWorldAxis(new THREE.Vector3(0,0,1), Math.PI / 8);
					}
					zRotateLeft = false;
				}else if(zRotateRight){
					if(group.rotation.z > (-Math.PI / 3)){
						group.rotateOnWorldAxis(new THREE.Vector3(0,0,1), -Math.PI / 8);
					}
					zRotateRight = false;
				}*/
				
				
				/* rotate about y axis
				if(yRotateLeft){
					if(group.rotation.y < (Math.PI / 3)){
						// clamp the rotation
						group.rotateOnWorldAxis(new THREE.Vector3(0,1,0), Math.PI / 8);
					}
					yRotateLeft = false;
				}else if(yRotateRight){
					if(group.rotation.y > (-Math.PI / 3)){
						group.rotateOnWorldAxis(new THREE.Vector3(0,1,0), -Math.PI / 8);
					}
					yRotateRight = false;
				}
				*/
				
				renderer.render(scene, camera);
			}
		});
		
	</script>



</html>