<html>

	<!-- 
		this page is for exploring manipulating vertices of a geometry and to 
		experiment with some ideas on how to do that given real-time facial landmark data.
	-->

	<head>
		<title>vstream display</title>
		<!--<script src="//cdnjs.cloudflare.com/ajax/libs/socket.io/2.2.0/socket.io.js" integrity="sha256-yr4fRk/GU1ehYJPAs8P4JlTgu0Hdsp4ZKrx8bDEDC3I=" crossorigin="anonymous"></script>-->
		<script src='https://cdnjs.cloudflare.com/ajax/libs/three.js/108/three.min.js'></script>
		<script src='testing_stuff/GLTFLoader.js'></script>
	</head>
	
	<style>
		#main{
			font-family: monospace;
			text-align: center;
			top: 5%;
			position: relative;
			display: block;
		}
		#container{
			width: 400px;
			height: 400px;
			margin: 0 auto;
			border: #000 solid 1px;
		}
	</style>
	
	<body>
		<div id='main'>
			<h3> vstream display </h3>
			
			<div id='container'>
			</div>
			
			<br />
			<br />
			
			<button id='nextAction'> action </button>
			
		</div>
	</body>


	<script>
	
		let loadedModels = [];
		const loader = new THREE.GLTFLoader();
		let avatarVertices = null;
		let avatarGeometry = null;
		let avatar;
		
		function extractVertices(vertices){
			//console.log(vertices.count);
			let vertexSet = [];
			// this for loop is wrong
			for(let i = 0; i <= vertices.count - 3; i += 3){
				let vertex = new THREE.Vector3();
				vertex.fromBufferAttribute(vertices, i);
				vertexSet.push(vertex);
			}
			return vertexSet;
		}
	
		function getModel(modelFilePath, name){
			return new Promise((resolve, reject) => {
				loader.load(
					modelFilePath,
					function(gltf){
						if(gltf.animations.length > 0){
							console.log(gltf.animations);
						}
						gltf.scene.traverse((child) => {

							if(child.type === "Mesh"){
								let material = child.material;
								let geometry = child.geometry;
								
								// https://stackoverflow.com/questions/52569738/how-to-access-single-vertices-of-mesh-loaded-with-gltfloader-in-three-js
								//console.log(geometry);
								avatarVertices = geometry.attributes.position.array;
								avatarGeometry = geometry;
								
								// assign color to each vertex 
								// https://threejs.org/examples/webgl_geometry_colors
								// https://stackoverflow.com/questions/10330342/threejs-assign-different-colors-to-each-vertex-in-a-geometry
								const colorOptions = [new THREE.Color(0xff0000), new THREE.Color(0x00ff00), new THREE.Color(0x0000ff)];
								const colorIdx = 0;
								
								let count = geometry.attributes.position.count;
								geometry.attributes['color'] = new THREE.BufferAttribute(new Float32Array(count * 3), 3);
								
								let colors = geometry.attributes.color;
								//console.log(geometry);
								
								for(let i = 0; i < count; i++){
									let currColor = colorOptions[(i % 3)];
									colors.setXYZ(i, currColor.r, currColor.g, currColor.b);
								}
								
								console.log(geometry);
								//material = new THREE.MeshBasicMaterial({ vertexColors: THREE.VertexColors });
								
								let obj = new THREE.Mesh(geometry, material);
								obj.scale.x = child.scale.x * 5;
								obj.scale.y = child.scale.y * 5;
								obj.scale.z = child.scale.z * 5;

								//obj.rotateOnAxis(new THREE.Vector3(0,1,0), Math.PI);
								//obj.rotateOnAxis(new THREE.Vector3(1,0,0), Math.PI);
								
								obj.name = name;
								resolve(obj);
							}else{
								//console.log(child.type);
							}
						});
					},
					// called while loading is progressing
					function(xhr){
						console.log( (xhr.loaded / xhr.total * 100) + '% loaded' );
					},
					// called when loading has errors
					function(error){
						console.log('An error happened');
						console.log(error);
					}
				);
			});
		}
			

		const el = document.getElementById("container");
		const renderer = new THREE.WebGLRenderer();
		const fov = 60;
		const camera = new THREE.PerspectiveCamera(fov, 1.0, 0.01, 1000);
		const scene = new THREE.Scene();
		scene.background = new THREE.Color(0xffffff);	
		
		renderer.shadowMap.enabled = true;
		renderer.setSize(400, 400);	
		el.appendChild(renderer.domElement);
		
		camera.position.set(0,2,20);
		scene.add(camera);

		let pointLight = new THREE.PointLight(0xffffff, 1, 0); //new THREE.pointLight( 0xffffff );
		pointLight.position.set(0, 10, 0);
		pointLight.castShadow = true;
		pointLight.shadow.mapSize.width = 512;
		pointLight.shadow.mapSize.height = 512;
		pointLight.shadow.camera.near = 10;
		pointLight.shadow.camera.far = 100;
		pointLight.shadow.camera.fov = 30;
		//scene.add(pointLight);

		// https://discourse.threejs.org/t/solved-glb-model-is-very-dark/6258
		// should always use a hemisphere light!
		var hemiLight = new THREE.HemisphereLight(0xffffff, 0x444444);
		hemiLight.position.set(0, 300, 0);
		scene.add(hemiLight);
		
		loadedModels.push(getModel('testing_stuff/basic_avatar_idea.glb', 'avatar'));
		Promise.all(loadedModels).then((objects) => {
			objects.forEach((mesh) => {
			
				var group = new THREE.Group();
				group.add(mesh);
			
				bgAxesHelper = new THREE.AxesHelper(10);
				group.add(bgAxesHelper);
			
				//mesh.castShadow = true;
				mesh.position.set(0,0,0);
				group.position.set(0,-12,7);
				avatar = group;
				
				scene.add(group);
				
				// have to do this weird set of rotations to get my avatar positioned correctly
				group.rotateOnAxis(new THREE.Vector3(0,0,1), Math.PI/2);
				group.rotateOnAxis(new THREE.Vector3(1,0,0), Math.PI/2);
				group.rotateOnAxis(new THREE.Vector3(0,1,0), -Math.PI/2);

				renderer.render(scene, camera);
			});
		});
		
		function moveVertex(vector3, x, y, z){
			vector3.set(x, y, z);
		}
		
		let currVertex = 0;
		function test1(){
			// try to stretch the avatar a bit
			// might be useful https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Iterators_and_Generators
			
			//let verts = extractVertices(avatarGeometry.attributes.position);
			//console.log(verts.length);
			
			let v = (currVertex++ % avatarGeometry.attributes.position.count)*3;
			avatarVertices[v] = avatarVertices[v] + 0.50;
			console.log("currently on vertex number: " + (currVertex - 1));
			/*
			for(let i = 0; i <= avatarVertices.length - 3; i += 3){
				avatarVertices[i] = avatarVertices[i] + 0.20;
				//avatarVertices[i+2] = avatarVertices[i+2] * 1.2;
				//avatarVertices[i+1] = avatarVertices[i+1] + 1.2;
			}*/			
			
			// https://stackoverflow.com/questions/20303239/three-js-how-to-update-buffergeometry-vertices
			// try this? https://stackoverflow.com/questions/36699389/verticesneedupdate-in-three-js/36699654#36699654
			// https://threejsfundamentals.org/threejs/lessons/threejs-custom-buffergeometry.html
			//avatarGeometry.dynamic = true;
			//avatarGeometry.attributes.position.dynamic = true;
			avatarGeometry.attributes.position.needsUpdate = true;
			avatarGeometry.attributes.color.needsUpdate = true;
			//console.log(avatarGeometry);
			renderer.render(scene, camera);
		}
		
		function moveLeftEye(){
			// ~index 11 - 17
			for(let i = 11; i < 17; i++){
				let index = i*3;
				avatarVertices[index] = avatarVertices[index] + 0.50;
			}
		}
		
		function moveRightEye(){
			// index ~92 - 100
			for(let i = 92; i <= 100; i++){
				let index = i*3;
				avatarVertices[index] = avatarVertices[index] + 0.50;			
			}
		}
		
		function closeRightEye(){
			// top eye verts (move them down)
			//[95, 96, 98] 
			[95, 96, 98].forEach((i) => {
				let index = i*3;
				avatarVertices[index+2] = avatarVertices[index+2] + 0.10;
			});
			
			// bottom eye verts (move them up)
			//[97, 99, 
			[97, 99].forEach((i) => {
				let index = i*3;
				avatarVertices[index+2] = avatarVertices[index+2] - 0.10;			
			});
		}
		
		function openRightEye(){
		}
		
		function moveMouth(){
			// index ~12 - 19
			for(let i = 12; i <= 19; i++){
				let index = i*3;
				avatarVertices[index] = avatarVertices[index] + 0.50;				
			}
		}



		document.getElementById('nextAction').addEventListener('click', (evt) => {
			//test1();
			//moveRightEye();
			//moveMouth();
			closeRightEye()
			avatar.rotateOnAxis(new THREE.Vector3(0,0,1), Math.PI/4); // degree of rotation can be a measure of 'sensitivity' maybe?
			
			avatarGeometry.attributes.position.needsUpdate = true;
			avatarGeometry.attributes.color.needsUpdate = true;
			renderer.render(scene, camera);
		});
		
		
		/*
		
			notes ----------------------
			
			7/29/20
			hmm this is pretty hard and I am getting sad. :/
			
			ok so maybe the server can be keeping track of relative changes and
			send commands over to the client to do them. hey, that sounds a lot like networking for games.
			
			i.e. do the eye vertices converge (i.e. eye closing).
			if so, then just send that action to the browser and the client-side 
			code can do the animation? 
			
			how can we take 2d coords and figure out what should be happening in 3d? is it even possible!?
		
			rotations should be not too bad right? we can start with that. i.e. if I rotate my head from one 
			side to the other, with just 2d coords can I know something about my head's rotation?
			
			alright, I decided to google it and found this: https://stackoverflow.com/questions/54368577/how-do-you-transform-2d-facial-landmarks-into-3d-world-coordinates
			
			:(, but not surprised. this looks helpful though: https://github.com/TadasBaltrusaitis/OpenFace
		
		*/
	
	</script>
	
</html>
