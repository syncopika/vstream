8/5/20

try this:

figure out some basic stuff first:
  - clamp angles
  - how are texures getting mapped to vertices in three.js?
    - need to load in and attach texture to each mesh? (i.e. https://discourse.threejs.org/t/textures-map-incorrectly-to-gltf-object/11544)
    - b/c wireframe movement doesn't seem to match up with texture movement when texure is applied


then try these things:
  - for the sake of getting things working, try mapping model vertices to landmark coords i.e.

    - we have landmark coords in landmark_data where each element is a coordinate
    - we have a bunch of meshes, each corresponding to a face part, and we know their vertices
      - manually (for now) find the ones to match with landmark coords until we can figure out a strategy to automate that?
    - in the end get something like:
	vertexMapping = {
		"mouthLeft": [index in landmark_data, index of the x-value of the corresponding vertex of the model],
                ...
        }
      hmm there's still something wrong with this idea. it would be really nice if I could say, given the current state
      of the landmark coords, move the mouth accordingly. then I'd just go through all the mouth landmark coords, find their 
      corresponding 3d vertices, and move them. 
 
      the next question is how much to move them, and how do I know where? 
      need some way to track previous state probably. 

      what about vertices that can't be mapped? how I deal with those? something about angles, vectors maybe?

      ooh big thing: figuring out how the coords in 2d land for the landmark coords can be correlated with the model's points in 3d land.'
		    how about this:
			- 'normalize' the 2d world that the landmark coords come from by making the center of the 2d canvas 0, 0.
			  then you just readjust all the coordinates based on the origin being at 0,0. so new x = width/2 - x if x < width/2.
			  then do the same for the y-coord.

basic algorithms as I have them now:
  - detecting forward/backwards (i.e. getting closer to webcam, further away):
	let dist = distance between the left jaw landmark coord and the right jaw landmark coord
        if dist gets smaller:
           move camera backwards
        else:
           move camera forwards
     - notice we manipulate the camera here

  - detecting side-to-side motion
	haven't gotten this one yet :<

  - detecting head tilt about z-axis (axis coming towards the camera)
	get angle between where a nose landmark coord was last time and now (assuming they both form line segments with the origin).
	if the angle is large enough:
		if angle > 0:
			rotate right 
		else:
			rotate left 

	- I think it might be good to maintain a current rotation 'normal' vector that'll be parallel with the nose. it could be useful
          when making lateral transformations at an angle I think (i.e. easily take the perpendicular vector of the normal and use that)

  - moving the mouth
	- 2-step process. move along the y-axis (up and down) then move along the x-axis (for now just the left and right corner of the mouth)
	- right now this is no good because any movement is dependent on the initial starting coordinates (what I called baseline).
	  it's bad because if the face moves from side-to-side or forwards/backwards, then that renders the initial baseline values useless.

	- maybe try this:
		we need to know just the initial distances/proportions at the beginning (calibration step?). those distances should be considered
		baseline and can be considered like the minimum boundary.
                then, since we have that info, we can just make adjustments to the mouth...how?

8/9/20
finally making progress. SHAPE KEYS ARE THE ANSWER! SHAPE KEYS ARE AMAZING!! :D
now I have to figure out how to refine some things and figure out how to deal with random spasms/jittering of the facial landmark coordinates.
also the eye movement is also jittery.

todo also: eyebrows. head rotation about y-axis. and x-axis?
for rotation about the y-axis, taking one of the left nose coords and the last coord in the middle nose part and tracking the distance between 
the 2 might be helpful.

8/10/20
interesting: https://github.com/mrdoob/three.js/issues/6465

8/12/20
idea for handling eye tracking/ moving eyeballs:
	have a separate mesh with an opening and overlay that on top of the eye area (which is just a section of the face mesh). 
        underneath the mesh with the opening will be the eyeball mesh. then join the opening mesh with the face mesh.
      	I'm thinking you can paint this section of the face mesh white to represent the sclera. we can use the opening mesh to 
        control eyelid movement as well with shape keys and be able to move the eyeball around without having to think about what happens
        when the eye closes/opens. (cause currently in my avatar the eye opening/closing is just the top and bottom vertices of the 
        eye mesh converging towards the middle. this will definitely not support eyeballs)

8/13/20
I think there might be a bug in the gltf loader? if you join 2 meshes in Blender and you don't name the planes, the count gets off by 
1 AND you might have 2 materials. for some reason, having the count be off and having more than 1 material caused the gltf loader
to load those joined meshes twice using the plane name (i.e. Plane.002 and Plane.002-1) renaming the planes so they were in sequential order and removing the additional material fixed the issue.

8/14/20
nevermind, no bug in the loader. the implementation is actually, I think, pretty good. the loader sees that my mesh has 2 materials and is able to split that mesh into the
corresponding meshes for each material and groups them into a group, with the group being the name of the mesh named in Blender. So basically the expected mesh becomes
a group when imported. 

8/15/20
experimented with interpolation and seems to work fairly well for the eyebrows! and I don't need to keep track of a prev value in this case. but need to do more testing. :D